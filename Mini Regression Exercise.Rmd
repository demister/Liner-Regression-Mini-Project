---
title: "Mini Regression Exercise"
author: "Dee Muralidharan"
date: "August 4, 2017"
output: html_document
---
## Set working directory
## ─────────────────────────

##   It is often helpful to start your R session by setting your working
##   directory so you don't have to type the full path names to your data
##   and other files

# set the working directory
# setwd("~/Desktop/Rstatistics")
# setwd("C:/Users/dataclass/Desktop/Rstatistics")

##   You might also start by listing the files in your working directory

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd() # where am I?
setwd ("D:/springboard Data science/Exercises/Machine Learning")
list.files("D:/springboard Data science/Exercises/Machine Learning")
```

## Load the states data


```{r cars}
states.data <- readRDS("D:/springboard Data science/Exercises/Machine Learning/states.rds") 
states.info <- data.frame(attributes(states.data)[c("names", "var.labels")])
tail(states.info, 8)
```

## Linear regression
## ═══════════════════

## Examine the data before fitting models
## ──────────────────────────────────────────

## Start by examining the data to check for problems.

```{r pressure, echo=FALSE}
# summary of expense and csat columns, all rows
sts.ex.sat <- subset(states.data, select = c("expense", "csat"))
summary(sts.ex.sat)
# correlation between expense and csat
cor(sts.ex.sat)
```

## Plot the data before fitting models
## ───────────────────────────────────────

##   Plot the data to look for multivariate outliers, non-linear
##   relationships etc.

```{r}
plot(sts.ex.sat)
```

## Linear regression example
## ─────────────────────────────

##   • Linear regression models can be fit with the `lm()' function
##   • For example, we can use `lm' to predict SAT scores based on
##     per-pupal expenditures:
# Fit our regression model

```{r}
# Fit our regression model
sat.mod <- lm(csat ~ expense, # regression formula
              data=states.data) # data set
# Summarize and print the results
summary(sat.mod) # show regression coefficients table
```

## Why is the association between expense and SAT scores /negative/?
## ─────────────────────────────────────────────────────────────────────

##   Many people find it surprising that the per-capita expenditure on
##   students is negatively related to SAT scores. The beauty of multiple
##   regression is that we can try to pull these apart. What would the
##   association between expense and SAT scores be if there were no
##   difference among the states in the percentage of students taking the
##   SAT?
## The lm class and methods
## ────────────────────────────

##   OK, we fit our model. Now what?
##   • Examine the model object:

```{r}
class(sat.mod)
names(sat.mod)
methods(class = class(sat.mod))[1:9]


```

# hist(residuals(sat.mod))

## Linear Regression Assumptions
## ─────────────────────────────────

##   • Ordinary least squares regression relies on several assumptions,
##     including that the residuals are normally distributed and
##     homoscedastic, the errors are independent and the relationships are
##     linear.

```{r}
##   • Investigate these assumptions visually by plotting your model:
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2)) #optional
plot(sat.mod, which = c(1, 2)) # "which" argument optional

```

## Comparing models
## ────────────────────

##   Do congressional voting patterns predict SAT scores over and above
##   expense? Fit two models and compare them:

```{r}
sat.voting.mod <-  lm(csat ~ expense + house + senate,
                      data = na.omit(states.data))
sat.mod <- update(sat.mod, data=na.omit(states.data))
# compare using the anova() function
anova(sat.mod, sat.voting.mod)
coef(summary(sat.voting.mod))
```

## Exercise: least squares regression
## ────────────────────────────────────────

```{r}
getwd() # where am I?
setwd ("D:/springboard Data science/Exercises/Machine Learning")
##list.files("D:/springboard Data science/Exercises/Machine Learning")
states.data <- readRDS("states.rds") 
states.info <- data.frame(attributes(states.data)[c("names", "var.labels")])
write.csv(states.data, "states.csv" )
tail(states.info, 8)
```

##   Use the /states.rds/ data set. Fit a model predicting energy consumed
##   per capita (energy) from the percentage of residents living in
##   metropolitan areas (metro). Be sure to
1. Examine/plot the data before fitting the model
```{r}
# summary of expense and csat columns, all rows

sts.ex.sat <- subset(states.data, select = c("energy", "metro"))
summary(sts.ex.sat)

## Plot energy and metro
plot(states.data$energy,states.data$metro)

# correlation between expense and csat

cor(na.omit(sts.ex.sat))

# There seems to be a possible negative correlation between energy consumed 
#and residents living in a metro area.
```

2. Print and interpret the model `summary'

```{r}

# Fit our regression model
sat.mod <- lm(energy ~ metro, # regression formula
              data=states.data) # data set
# Summarize and print the results
summary(sat.mod) # show regression coefficients table
plot(sat.mod)
cor(na.omit (stat.mod))
```

It can be inferred that the correlation is very poor as the Adjusted R-squared:  0.097. The contribution of metro potulation to the liner model of energy use is very poor.

Will Try to find the best corellation that would benefit the energy usage column
```{r}
## Tidy data from the file 1. Remove rows with na's. 2.Remove state and regions
##install.packages("dplyr")
library(dplyr)
##install.packages("tidyr")
library(tidyr)
states.data1 <- states.data %>% drop_na(pop)
states.data1 <- states.data %>% drop_na(green)
states.data1 <- states.data1 %>% select(-state, -region)
write.csv(states.data1, "D:/springboard Data science/Exercises/Machine Learning/states.csv" )
cor(states.data1)
```

It can be found using the corr function that state "area", "toxic" , "Green" and "house" have the best corellations and hence are co-leniar

              energy      
pop       -0.184035749 
area    ##0.662651876
density   -0.328420337 
metro     -0.339744521 
waste     -0.252649916 
energy    1.000000000  
miles     0.233889397  
toxic   ##0.562452402  
green  ##0.7706181        
house  ##-0.634687245 
senate  -0.477318891 
csat     0.164901350  
vsat     0.187582680  
msat     0.141883149 
percent -0.304003280 
expense -0.006950791 
income  -0.143685178 
high     0.039718282 
college -0.224146236 

 Lets try and create a regression model using these values and study the best fit and the r2 values
 
```{r}
stat.mod.energy <-  lm(energy ~ pop + area+ +density +metro + waste + miles + toxic + green + house + expense + income, # regression formula
              data=states.data1)
summary(stat.mod.energy)
plot(stat.mod.energy)
```
```{r}
stat.mod.energy2 <-  lm(energy ~   area+ + toxic + green  , # regression formula
              data=states.data1)
summary(stat.mod.energy2)
plot(stat.mod.energy2)
```



This model (stat.mod.energy2) has a better Adjusted r2 value (0.7652) compared to the previous models (stat.mod.energy) adjusted r2 value of 0.7356, hence is a better fit. It was also found that omitting house all the irrevelant values increase the r2 value.  

## Interactions and factors
## ══════════════════════════

## Modeling interactions
## ─────────────────────────

##   Interactions allow us assess the extent to which the association
##   between one predictor and the outcome depends on a second predictor.
##   For example: Does the association between expense and SAT scores
##   depend on the median income in the state?
 

## Modeling interactions

## ─────────────────────────

##   Interactions allow us assess the extent to which the association
##   between one predictor and the outcome depends on a second predictor.
##   For example: Does the association between expense and SAT scores
##   depend on the median income in the state?

  #Add the interaction to the model
  
##   1. Add on to the regression equation that you created in exercise 1 by

##      generating an interaction term and testing the interaction

```{r}
  #Add the interaction to the model
 
energy.inter <- lm(energy ~ toxic * green, data = states.data1)
#Show the results
  coef(summary(energy.inter)) # show regression coefficients table
  summary(energy.inter)
```
##   2. Try adding region to the model. Are there significant differences
##      across the four regions?

```{r}

# Generate and test this regression again with region in the model
model.states.intra <- lm(energy ~ green * toxic + region, data =  na.omit(states.data))
# make sure R knows region is categorical
str(states.data$region)
states.data$region <- factor(states.data$region)
#Add region to the model
sat.region <- lm(energy ~ region, data=states.data)
summary(sat.region)
```

 
##There does appear to be differences between regions.  It seems the South is correlatedwith higher energy use while the Northeast and Midwest are negatively correlated with energy use. The addition of region increased the R squared on the model even further.
 
 
```{r}
model.ex2 <- lm(energy ~ metro*green + region, data = na.omit(states.data))
summary(model.ex2)

```
